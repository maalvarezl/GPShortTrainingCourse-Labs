{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 5: Dimensionality Reduction using Gaussian Processes\n",
    "\n",
    "_written by Wil Ward, adapted from notebooks by Max Zwiessele and [Neil Lawrence](http://inverseprobability.com/)_\n",
    "\n",
    "This lab focusses on using Gaussian processes for unsupervised analysis. We will apply dimensionality reduction to a dataset of images and compare it to Gaussian process latent variable modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Getting Started\n",
    "\n",
    "We import all the relevant packages, as in previous lab sessions, and create a utility for plotting our models (note this is different to the `plot_gp()` function used in previous labs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support for maths\n",
    "import numpy as np\n",
    "# Plotting tools\n",
    "from matplotlib import pyplot as plt\n",
    "# we use the following for plotting figures in jupyter\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# GPy: Gaussian processes library\n",
    "import GPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will to order our colours for different classes (colourblind friendly)\n",
    "colours = [\"#B35806\", \"#F1A340\", \"#FEE0B6\", \"#D8DAEB\", \"#998EC3\", \"#542788\"]#, \"#6DDA4C\", \"#85831F\", \"#B36A29\", \"#CF4E4A\"]\n",
    "\n",
    "def plot_model(X, labels, which_dims=None):\n",
    "    # Prepare dimensions of data\n",
    "    X = X[:,:] if which_dims is None else X[:,which_dims]\n",
    "    # Unique labels\n",
    "    ulabs = []\n",
    "    for lab in labels:\n",
    "        if lab not in ulabs:\n",
    "            ulabs.append(lab)\n",
    "    # Prepare figure env.\n",
    "    plt.figure()\n",
    "    for i, lab in enumerate(ulabs):\n",
    "        plt.scatter(*X[labels==lab].T, marker='o', color=colours[i], label=lab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lab, we will also use a dataset of handwritten numbers, $0,\\ldots,9$. We will use a subset of the digits in the lab but this can be extended to use all of them desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPy.util.datasets.authorize_download = lambda x: True # This gives GPy permission to download the dataset\n",
    "\n",
    "# We only select these digits to work with\n",
    "these_digits = [0, 1, 2, 6, 7, 9]\n",
    "# Download the data, cache it locally and pass to variable `data`.\n",
    "data = GPy.util.datasets.decampos_digits(which_digits=these_digits)\n",
    "\n",
    "print(\"\\nData keys:\")\n",
    "print(data.keys())\n",
    "\n",
    "print(\"\\nCitation:\")\n",
    "print(data['citation'])\n",
    "\n",
    "print(\"\\nInfo:\")\n",
    "print(data['info'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note we take our data, which exists only as the observed digits as `16x16` images. Each one has a corresponding label, indicating the number shown in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data['Y']\n",
    "labels = data['str_lbls'].flatten()\n",
    "\n",
    "print(y.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will plot a random selection of the digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axs = plt.subplots(3,3,figsize=(14,14))\n",
    "for i,k in enumerate(np.random.randint(0, 329, size=(9,1))):\n",
    "    axs[int(np.floor(i/3)), i%3].matshow(np.reshape(y[k,:],(16,16)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Principal Component Analysis\n",
    "\n",
    "Principal component analysis (PCA) finds a rotation of the observed outputs, such that the rotated principal component (PC) space maximises the variance of the data observed, sorted from most to least important. Here, we define importance as the variablity of a corresponding PC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a PCA class with the digits dataset\n",
    "p = GPy.util.pca.PCA(y)\n",
    "\n",
    "p.plot_fracs(20); # We plot the first 20 eigenvalue fractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.plot_2d(y, labels=labels, colors=colours) # We plot each digit according to its first two PCs\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see here that we can distinguish populations for most classes, though there is no necessarily a clear distinction for digits $1$ and $2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Gaussian Process Latent Variable Model\n",
    "The Gaussian process latent variable model (GP-LVM) embeds a PCA into a Gaussian process framework, where the latent inputs $\\mathbf{X}$ are learnt as hyperparameters and the mapping variables  $\\mathbf{W}$ are integrated out. The advantage of this interpretation is that it allows PCA to be generalised in a non-linear way by replacing the resulting _linear_ covariance with a non-linear covariance.\n",
    "\n",
    "We can see how GP-LVM is equivalent to PCA using an automatic relevance determination (ARD) linear kernel. We will define the latent space with $D=4$ dimensions. We can use `GPy` and its `GPLVM` model to perform this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARD linear kernel\n",
    "k = GPy.kern.Linear(4, ARD=True)\n",
    "\n",
    "# GPLVM with 4D latent space\n",
    "m = GPy.models.GPLVM(y, input_dim=4, kernel=k)\n",
    "\n",
    "# Optimise model\n",
    "m.optimize(messages=1, max_iters=1000) \n",
    "# Preview our model\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the contribution of ARD in our respective latent dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.kern.plot_ARD();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now compare the results of the linear ARD kernel with the PCA solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(m.X, labels, m.linear.variances.argsort()[-2:])\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the solution with a linear kernel is the same as that of the PCA, with some minor rotational changes. For the sake of time, the solution was only run for a maximum of 1000 iterations, though it converged before this. For the linear covariance, the latent points can be optimised with an eigenvalue problem but in the general case, particularly for non-linear covariance functions, gradient-based optimisation is needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "(a) Try performing the GP-LVM with a non-linear covariance function, for example with the `RBF` kernel. How does the non-linear model differ from the linear one? Are there digits that the GP-LVM with non-linear covariance can seperate but that PCA is not able to?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Try modifying the covariance function using additive or multiplicative combinations. How does this affect the result?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Bayesian GP-LVM\n",
    "\n",
    "In GP-LVM, we used a point estimate of the distribution of the latent inputs $\\mathbf{X}$. The estimate is derived through maximum likelihood or through a maximum _a posteriori_ (MAP) approach. Ideally, we would like to estimate a distribution over the input $\\mathbf{X}$. In the _Bayesian_ GP-LVM, we approximate the true distribution $p(\\mathbf{X}|\\mathbf{y})$ using a variational approximation $q(\\mathbf{X})$ and then integrating $\\mathbf{X}$ out.\n",
    "\n",
    "Approimating the posterior in this way allows us to optimise a lower bound on the marginal likelihood. Handling the uncertainty in a principled way allows the model to make an assessment of whether a particular latent dimension is required, or the variation is better explained by noise. This allows the algorithm to \"_switch off_\" latent dimensions. This switching off can be time consuming, and may take a while to optimise, so we use a sparse approximation.\n",
    "\n",
    "We assume a 5-D space for the latent inputs, and set the number of inducing points for our sparse approximation to $M=25$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = GPy.kern.RBF(5, ARD=True) # Non-linear ARD kernel\n",
    "\n",
    "# Bayesian GPLVM model\n",
    "m = GPy.models.BayesianGPLVM(y, input_dim=5, kernel=k, num_inducing=25)\n",
    "\n",
    "# Optimise\n",
    "m.optimize(messages=1, max_iters=2500)\n",
    "\n",
    "# Preview the model\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we plot the contribution of ARD in our respective latent dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.kern.plot_ARD();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And plot the two most _important_ dimensions of the means of the latent inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(m.X.mean, labels, m.rbf.lengthscale.argsort()[:2])\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "How does the Bayesian GP-LVM compare with the standard GP-LVM model?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
